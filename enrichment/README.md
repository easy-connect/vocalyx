# ğŸ¨ Module d'Enrichissement Vocalyx

Le module d'enrichissement gÃ©nÃ¨re automatiquement des insights exploitables Ã  partir des transcriptions audio via un LLM local (Large Language Model).

## âœ¨ FonctionnalitÃ©s

Pour chaque transcription, le module gÃ©nÃ¨re :

- ğŸ“Œ **Titre** : Un titre court et pertinent (10 mots max)
- ğŸ“ **RÃ©sumÃ©** : SynthÃ¨se en 2-3 phrases
- ğŸ”¹ **Points clÃ©s** : 3-5 points principaux extraits
- ğŸ˜Š **Sentiment** : Analyse du ton (positif/nÃ©gatif/neutre/mixte)
- ğŸ·ï¸ **Topics** : ThÃ¨mes principaux (optionnel)

## ğŸš€ Installation

### 1. Installer les dÃ©pendances

```bash
# Via make (recommandÃ©)
make install-enrichment

# Ou manuellement
pip install -r requirements-enrichment.txt
```

### 2. TÃ©lÃ©charger le modÃ¨le LLM

Le module utilise [Mistral 7B Instruct](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF) par dÃ©faut (~4GB).

```bash
# Via make (recommandÃ©)
make download-model

# Ou manuellement
mkdir -p models
cd models
wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q4_K_M.gguf
```

### 3. Configurer

Ajouter la section `[ENRICHMENT]` dans `config.ini` :

```bash
# Automatiquement
bash scripts/add_enrichment_config.sh

# Ou manuellement (voir config.ini.exemple)
```

### 4. CrÃ©er les tables de la base de donnÃ©es

```bash
make db-migrate

# Ou manuellement
python3 -c "from enrichment.models import create_tables; create_tables()"
```

## ğŸ¯ Utilisation

### Lancer le worker

```bash
# Via make (recommandÃ©)
make run-enrichment

# Ou directement
python3 run_enrichment.py
```

Le worker va :
1. âœ… Charger le modÃ¨le LLM (30-60s au dÃ©marrage)
2. ğŸ”„ Interroger la DB toutes les 15s pour trouver les nouvelles transcriptions
3. ğŸ¨ GÃ©nÃ©rer les enrichissements (30-60s par transcription)
4. ğŸ’¾ Sauvegarder les rÃ©sultats dans la table `enrichments`

### Lancer API + Worker ensemble

```bash
make run-all
```

### Mode test

```bash
# Test complet du module
python3 test_enrichment_module.py

# Test rapide avec une transcription
python3 -c "from enrichment.worker import test_enrichment; test_enrichment()"
```

## âš™ï¸ Configuration

### Configuration de base (`config.ini`)

```ini
[ENRICHMENT]
# Activer/dÃ©sactiver
enabled = true

# Worker
poll_interval_seconds = 15  # FrÃ©quence de vÃ©rification
batch_size = 3              # Nombre de transcriptions par batch

# ModÃ¨le LLM
model_path = models/mistral-7b-instruct-v0.3.Q4_K_M.gguf
n_ctx = 4096               # Taille du contexte
n_threads = 6              # Threads CPU

# GÃ©nÃ©ration
temperature = 0.3          # CrÃ©ativitÃ© (0.0 = dÃ©terministe)
max_tokens = 500           # Longueur max de la gÃ©nÃ©ration

# FonctionnalitÃ©s
generate_title = true
generate_summary = true
generate_bullets = true
generate_sentiment = true
generate_topics = false    # Plus lent
```

### Presets recommandÃ©s

#### ğŸš€ Vitesse (tests, gros volumes)
```ini
n_ctx = 2048
n_threads = 8
temperature = 0.2
max_tokens = 300
batch_size = 5
```

#### âš–ï¸ Ã‰quilibrÃ© (production standard)
```ini
n_ctx = 4096
n_threads = 6
temperature = 0.3
max_tokens = 500
batch_size = 3
```

#### ğŸ¯ QualitÃ© maximale
```ini
model_path = models/mistral-7b-instruct-v0.3.Q5_K_M.gguf  # ModÃ¨le plus prÃ©cis
n_ctx = 8192
temperature = 0.4
max_tokens = 700
batch_size = 1
generate_topics = true
```

## ğŸ“Š Architecture

```
enrichment/
â”œâ”€â”€ __init__.py          # Exports du module
â”œâ”€â”€ config.py            # Configuration
â”œâ”€â”€ models.py            # ModÃ¨les SQLAlchemy (table enrichments)
â”œâ”€â”€ llm_engine.py        # Wrapper llama-cpp-python
â”œâ”€â”€ prompts.py           # Templates de prompts
â”œâ”€â”€ processors.py        # Logique d'enrichissement
â”œâ”€â”€ utils.py             # Utilitaires (parsing, validation, etc.)
â”œâ”€â”€ worker.py            # Worker principal
â””â”€â”€ README.md            # Ce fichier
```

### Flux de donnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transcription  â”‚  (status=done, enrichment_requested=1)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Worker      â”‚  (polling toutes les 15s)
â”‚  - RÃ©cupÃ¨re     â”‚
â”‚  - Traite       â”‚
â”‚  - Sauvegarde   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Engine    â”‚  (llama-cpp-python)
â”‚  - Charge model â”‚
â”‚  - GÃ©nÃ¨re texte â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Enrichment    â”‚  (titre, rÃ©sumÃ©, bullets, sentiment)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—„ï¸ SchÃ©ma de la base de donnÃ©es

### Table `enrichments`

| Colonne | Type | Description |
|---------|------|-------------|
| `id` | Integer | ClÃ© primaire |
| `transcription_id` | String | FK vers transcriptions.id |
| `status` | String | pending, processing, done, error |
| `title` | Text | Titre gÃ©nÃ©rÃ© |
| `summary` | Text | RÃ©sumÃ© |
| `bullets` | JSON | Liste de points clÃ©s |
| `sentiment` | String | positif, negatif, neutre, mixte |
| `sentiment_confidence` | Float | Confiance 0-1 |
| `topics` | JSON | Liste de topics (optionnel) |
| `model_used` | String | Nom du modÃ¨le |
| `generation_time` | Float | Temps de gÃ©nÃ©ration (s) |
| `tokens_generated` | Integer | Nombre de tokens gÃ©nÃ©rÃ©s |
| `retry_count` | Integer | Nombre de tentatives |
| `last_error` | Text | Dernier message d'erreur |
| `created_at` | DateTime | Date de crÃ©ation |
| `started_at` | DateTime | DÃ©but du traitement |
| `finished_at` | DateTime | Fin du traitement |

## ğŸ“¡ API

Pour rÃ©cupÃ©rer les enrichissements via l'API :

```bash
# Obtenir une transcription + son enrichissement
GET /api/transcribe/{transcription_id}

# RÃ©ponse
{
  "id": "...",
  "text": "...",
  "enrichment": {
    "title": "RÃ©clamation client - Produit dÃ©fectueux",
    "summary": "Le client appelle pour signaler un problÃ¨me...",
    "bullets": [
      "Produit dÃ©fectueux reÃ§u",
      "Demande de remboursement",
      "Urgence exprimÃ©e"
    ],
    "sentiment": "negatif",
    "sentiment_confidence": 0.85
  }
}
```

## ğŸ§ª Tests

### Test complet du module

```bash
python3 test_enrichment_module.py
```

Tests effectuÃ©s :
1. âœ… Configuration
2. âœ… ModÃ¨les de base de donnÃ©es
3. âœ… Utilitaires
4. âœ… Templates de prompts
5. âœ… Moteur LLM
6. âœ… Processeur
7. âœ… Worker

### Test rapide

```bash
# Test avec une transcription fictive
python3 -c "from enrichment.worker import test_enrichment; test_enrichment()"
```

### Tests unitaires des utilitaires

```bash
python3 enrichment/utils.py
```

## ğŸ”§ DÃ©pannage

### Le modÃ¨le ne se charge pas

**Erreur** : `FileNotFoundError: models/mistral-7b-instruct-v0.3.Q4_K_M.gguf`

**Solution** :
```bash
make download-model
# Ou vÃ©rifier le chemin dans config.ini
```

### GÃ©nÃ©ration trÃ¨s lente

**Cause** : Trop peu de threads ou modÃ¨le trop gros

**Solution** :
```ini
[ENRICHMENT]
n_threads = 8  # Augmenter
# Ou utiliser un modÃ¨le plus petit (Q4 au lieu de Q5)
```

### Erreur "Out of memory"

**Cause** : RAM insuffisante

**Solution** :
```ini
[ENRICHMENT]
n_ctx = 2048      # RÃ©duire le contexte
batch_size = 1    # Traiter une Ã  la fois
# Ou utiliser un modÃ¨le plus petit (tiny au lieu de small)
```

### Enrichissements de mauvaise qualitÃ©

**Solutions** :
```ini
[ENRICHMENT]
temperature = 0.2           # Plus dÃ©terministe
max_tokens = 700            # Plus de place pour gÃ©nÃ©rer
beam_size = 7               # Meilleur dÃ©codage (si supportÃ©)
# Ou utiliser un modÃ¨le plus gros (Q5 au lieu de Q4)
```

### Worker ne trouve pas de transcriptions

**VÃ©rifications** :
```sql
-- VÃ©rifier qu'il y a des transcriptions Ã  enrichir
SELECT COUNT(*) FROM transcriptions 
WHERE status='done' AND enrichment_requested=1;

-- VÃ©rifier les enrichissements existants
SELECT COUNT(*) FROM enrichments;
```

**Solution** :
```python
# Forcer l'enrichissement d'une transcription
from database import SessionLocal, Transcription
db = SessionLocal()
trans = db.query(Transcription).filter(Transcription.status=='done').first()
trans.enrichment_requested = 1
db.commit()
```

## ğŸ“ˆ Performance

### Temps de traitement attendus

| ModÃ¨le | CPU | Temps/transcription | Vitesse |
|--------|-----|---------------------|---------|
| **Mistral 7B Q4** | i7 8 cores | 30-40s | ~15 tok/s |
| **Mistral 7B Q5** | i7 8 cores | 40-60s | ~10 tok/s |
| **Llama 2 7B Q4** | i7 8 cores | 40-50s | ~12 tok/s |

### Optimisations

- âœ… **Augmenter `n_threads`** : Utiliser plus de cÅ“urs CPU
- âœ… **RÃ©duire `n_ctx`** : Moins de contexte = plus rapide
- âœ… **Utiliser Q4 au lieu de Q5** : ModÃ¨le plus lÃ©ger
- âœ… **Augmenter `batch_size`** : Traiter plusieurs en parallÃ¨le
- âœ… **DÃ©sactiver `generate_topics`** : Gain de ~10s

## ğŸŒ Langues supportÃ©es

### ModÃ¨les recommandÃ©s par langue

| Langue | ModÃ¨le recommandÃ© |
|--------|-------------------|
| ğŸ‡«ğŸ‡· FranÃ§ais | Mistral 7B Instruct v0.3 |
| ğŸ‡¬ğŸ‡§ Anglais | Mistral 7B Instruct v0.3 |
| ğŸ‡ªğŸ‡¸ Espagnol | Mistral 7B Instruct v0.3 |
| ğŸ‡©ğŸ‡ª Allemand | Mistral 7B Instruct v0.3 |

Pour changer la langue :
```ini
[ENRICHMENT]
prompt_language = en
output_language = en
```

## ğŸ” SÃ©curitÃ© & ConfidentialitÃ©

- âœ… **ModÃ¨le local** : Aucune donnÃ©e envoyÃ©e Ã  des API externes
- âœ… **Offline** : Fonctionne sans connexion internet (aprÃ¨s tÃ©lÃ©chargement du modÃ¨le)
- âœ… **RGPD compatible** : Toutes les donnÃ©es restent sur votre serveur

## ğŸ“š Ressources

- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - Wrapper Python utilisÃ©
- [Mistral AI](https://mistral.ai/) - CrÃ©ateur du modÃ¨le Mistral
- [TheBloke sur HuggingFace](https://huggingface.co/TheBloke) - ModÃ¨les quantizÃ©s GGUF

## ğŸ†˜ Support

- **Documentation** : [docs/README.md](../docs/README.md)
- **Issues** : GitHub Issues
- **Email** : guilhem.l.richard@gmail.com

---

**Version 1.0.0** | Module d'enrichissement Vocalyx